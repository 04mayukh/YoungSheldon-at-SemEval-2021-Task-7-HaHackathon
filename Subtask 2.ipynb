{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Haha_2 github final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV89B_Ic1ByU"
      },
      "source": [
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_8XAulY1FkP"
      },
      "source": [
        "!pip install transformers==4.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyskhoTc1Kr5"
      },
      "source": [
        "pip install tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQika3il0lFt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import os\n",
        "from collections import Counter\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "\n",
        "import ekphrasis\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xw_CJeY0_tj"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n",
        "\r\n",
        "tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihW0zZI60lF1"
      },
      "source": [
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=True,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9vqcN8P0lF3"
      },
      "source": [
        "df_train = pd.read_csv('/content/train.csv', encoding='utf-8')\n",
        "df_train['humor_rating'] = df_train['humor_rating'].fillna(0)\n",
        "df_train['humor_controversy'] = df_train['humor_controversy'].fillna(2)\n",
        "text_train = df_train[\"text\"]\n",
        "df_train.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G2-pOts0lF3"
      },
      "source": [
        "df_val = pd.read_csv('/content/dev.csv', encoding='utf-8')\n",
        "df_val['humor_rating'] = df_val['humor_rating'].fillna(0)\n",
        "df_val['humor_controversy'] = df_val['humor_controversy'].fillna(2)\n",
        "text_val = df_val[\"text\"]\n",
        "df_val.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vTg2clLsK9k"
      },
      "source": [
        "df_test = pd.read_csv('/content/public_test.csv', encoding='utf-8')\r\n",
        "text_test = df_test[\"text\"]\r\n",
        "df_test.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJYpGI3I0lF4"
      },
      "source": [
        "print(len(text_train))\n",
        "print(len(text_val))\n",
        "print(len(text_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3xacQZq0lF4"
      },
      "source": [
        "def print_text(texts,i,j):\n",
        "    for u in range(i,j):\n",
        "        print(texts[u])\n",
        "        print()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICRzVCMB0lF5"
      },
      "source": [
        "print_text(text_train,0,10)\n",
        "print(\"##############################################################################################################\")\n",
        "print_text(text_val,0,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi7Mcmlv0lF5"
      },
      "source": [
        "# Functions for chat word conversion\n",
        "f = open(\"slang.txt\", \"r\")\n",
        "chat_words_str = f.read()\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXm_BGpc0lF6"
      },
      "source": [
        "# Chat word conversion\n",
        "# Training set\n",
        "text_train = text_train.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_train,0,10)\n",
        "\n",
        "print(\"********************************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_val = text_val.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_val,0,10)\n",
        "\n",
        "# Test set\n",
        "text_test = text_test.apply(lambda text: chat_words_conversion(text))\n",
        "# print_text(text_test,0,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1GiyEH0lF6"
      },
      "source": [
        "def ekphrasis_pipe(sentence):\n",
        "    cleaned_sentence = \" \".join(text_processor.pre_process_doc(sentence))\n",
        "    return cleaned_sentence"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGbdHvIX0lF7"
      },
      "source": [
        "# Training set\n",
        "text_train = text_train.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Training set completed.......\")\n",
        "#Validation set\n",
        "text_val = text_val.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Validation set completed.......\")\n",
        "#Test set\n",
        "text_test = text_test.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWsDbRPo0lF7"
      },
      "source": [
        "# Finding length of longest array\n",
        "maxLen = len(max(text_test,key = lambda text: len(text.split(\" \"))).split(\" \"))\n",
        "print(maxLen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChqDR4Sd0lF7"
      },
      "source": [
        "u = lambda text: len(text.split(\" \"))\n",
        "sentence_lengths = []\n",
        "for x in text_test:\n",
        "    sentence_lengths.append(u(x))\n",
        "print(sorted(sentence_lengths)[-200:])\n",
        "print(len(sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzpL4nwN0lF8"
      },
      "source": [
        "is_humor = df_train[\"is_humor\"]\n",
        "humor_rating = df_train[\"humor_rating\"]\n",
        "humor_controversy = df_train[\"humor_controversy\"].astype(int)\n",
        "offense_rating = df_train[\"offense_rating\"]\n",
        "print(Counter(is_humor))\n",
        "print(Counter(humor_controversy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXVkP4b754M_"
      },
      "source": [
        "is_humor_val = df_val[\"is_humor\"]\r\n",
        "humor_rating_val = df_val[\"humor_rating\"]\r\n",
        "humor_controversy_val = df_val[\"humor_controversy\"].astype(int)\r\n",
        "offense_rating_val = df_val[\"offense_rating\"]\r\n",
        "print(Counter(is_humor_val))\r\n",
        "print(Counter(humor_controversy_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46UA6xX20lF9"
      },
      "source": [
        "from transformers import RobertaTokenizerFast, TFRobertaModel, TFBertModel, BertTokenizerFast, ElectraTokenizerFast, TFElectraModel, AlbertTokenizerFast, TFAlbertModel, XLNetTokenizerFast, TFXLNetModel, MPNetTokenizerFast, TFMPNetModel\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oujy0A_J0lF-"
      },
      "source": [
        "# USe different tokeniser as required\r\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hysOrYmC0lF-"
      },
      "source": [
        "text_train = list(text_train)\n",
        "text_val = list(text_val)\n",
        "text_test = list(text_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-C2A4qS0lF-"
      },
      "source": [
        "train_encodings = tokenizer(text_train, max_length=150, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "val_encodings = tokenizer(text_val, max_length=150, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "test_encodings = tokenizer(text_test, max_length=150, truncation=True, padding=\"max_length\", return_tensors='tf')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVxYK_rC0lF-"
      },
      "source": [
        "print(np.shape(train_encodings[\"input_ids\"]))\n",
        "print(np.shape(val_encodings[\"input_ids\"]))\n",
        "print(np.shape(test_encodings[\"input_ids\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW5TfuDz0lF_"
      },
      "source": [
        "print(train_encodings[\"input_ids\"][0])\n",
        "print(\"***************************************************************************\")\n",
        "print(val_encodings[\"input_ids\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QUtPKK0lGA"
      },
      "source": [
        "def hahackathon_task_2(input_shape):\n",
        "    # Import model as required\n",
        "    model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "    layer = model.layers[0]\n",
        "    #Model\n",
        "    inputs = keras.Input(shape=input_shape, dtype='int32')\n",
        "    input_masks = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "    outputs = layer([inputs, input_masks])\n",
        "    output = outputs[0]\n",
        "    # pooled_output = outputs[1]\n",
        "    pooled_output = output[:, 0, :] # Use for bert, roberta, albert, mpnet, electra\n",
        "    # pooled_output = output[:, -1] #Use for XLNet\n",
        "\n",
        "    # Humour regression\n",
        "    humor_reg = layers.Dropout(0.3)(pooled_output)\n",
        "    humor_reg = layers.Dense(1)(humor_reg)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs,input_masks], outputs=humor_reg, name='task_2')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4yJuStxv_jR"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqS2O31g34V4"
      },
      "source": [
        "with strategy.scope():\r\n",
        "    model = hahackathon_task_2((150,))\r\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=3e-5)\r\n",
        "    loss_fun = [\r\n",
        "          tf.keras.losses.LogCosh()\r\n",
        "    ]\r\n",
        "    metric = [\r\n",
        "        tf.keras.metrics.RootMeanSquaredError()\r\n",
        "    ]\r\n",
        "    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyHqVLrVSutV"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfVFStvq0lGC"
      },
      "source": [
        "checkpoint = ModelCheckpoint(filepath='/content/task-2.{epoch:03d}.h5',\n",
        "                                 verbose = 0,\n",
        "                                 save_weights_only=True,\n",
        "                                 epoch=4)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaZVm7y0L2xh"
      },
      "source": [
        "# albert base Pooled output\r\n",
        "history_task_1_b = model.fit(\r\n",
        "    x = [train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"]],\r\n",
        "    y = offense_rating,\r\n",
        "    validation_data = ([val_encodings[\"input_ids\"],val_encodings[\"attention_mask\"]], offense_rating_val),\r\n",
        "    callbacks = [checkpoint],\r\n",
        "    batch_size=16,\r\n",
        "    shuffle=True,\r\n",
        "    epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6vdbd1c0lGD"
      },
      "source": [
        "val_answer = model.predict([val_encodings[\"input_ids\"],val_encodings[\"attention_mask\"]])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiA7vnDzHORQ"
      },
      "source": [
        "val_answer = np.where(val_answer<0, 0, val_answer)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhA_6XYZXPdy"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\r\n",
        "from math import sqrt\r\n",
        "sqrt(mean_squared_error(offense_rating_val, val_answer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9PnLwm_YDm9"
      },
      "source": [
        "test_answer = model.predict([test_encodings[\"input_ids\"],test_encodings[\"attention_mask\"]])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbDRLcXetfN4"
      },
      "source": [
        "test_2 = []\r\n",
        "for i in range(0,len(test_answer)):\r\n",
        "  test_2.append(test_answer[i][0])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jd0f0GP2NT_"
      },
      "source": [
        "len(test_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv79OhZM1GKx"
      },
      "source": [
        "test_id = df_test[\"id\"]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2-3QLOy1CBS"
      },
      "source": [
        "test_dict = {\r\n",
        "    \"id\" : test_id,\r\n",
        "    \"offense_rating\" : test_2\r\n",
        "}"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kYwYFUr1aiX"
      },
      "source": [
        "df_test = pd.DataFrame(test_dict)\r\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaLnkpg-2X3E"
      },
      "source": [
        "df_test.to_csv('albert-2.csv', index=False)"
      ],
      "execution_count": 50,
      "outputs": []
    }
  ]
}