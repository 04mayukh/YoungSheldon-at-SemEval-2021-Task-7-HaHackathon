{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Haha_1_a github final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV89B_Ic1ByU"
      },
      "source": [
        "!pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_8XAulY1FkP"
      },
      "source": [
        "!pip install transformers==4.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyskhoTc1Kr5"
      },
      "source": [
        "pip install tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQika3il0lFt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import os\n",
        "from collections import Counter\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "\n",
        "import ekphrasis\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xw_CJeY0_tj"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n",
        "\r\n",
        "tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihW0zZI60lF1"
      },
      "source": [
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=True,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9vqcN8P0lF3"
      },
      "source": [
        "df_train = pd.read_csv('/content/train.csv', encoding='utf-8')\n",
        "df_train['humor_rating'] = df_train['humor_rating'].fillna(0)\n",
        "df_train['humor_controversy'] = df_train['humor_controversy'].fillna(2)\n",
        "text_train = df_train[\"text\"]\n",
        "df_train.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G2-pOts0lF3"
      },
      "source": [
        "df_val = pd.read_csv('/content/dev.csv', encoding='utf-8')\n",
        "df_val['humor_rating'] = df_val['humor_rating'].fillna(0)\n",
        "df_val['humor_controversy'] = df_val['humor_controversy'].fillna(2)\n",
        "text_val = df_val[\"text\"]\n",
        "df_val.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DlL1_I8nlf_"
      },
      "source": [
        "df_test = pd.read_csv('/content/public_test.csv', encoding='utf-8')\r\n",
        "text_test = df_test[\"text\"]\r\n",
        "df_test.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJYpGI3I0lF4"
      },
      "source": [
        "print(len(text_train))\n",
        "print(len(text_val))\n",
        "print(len(text_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3xacQZq0lF4"
      },
      "source": [
        "def print_text(texts,i,j):\n",
        "    for u in range(i,j):\n",
        "        print(texts[u])\n",
        "        print()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICRzVCMB0lF5"
      },
      "source": [
        "print_text(text_train,0,10)\n",
        "print(\"##############################################################################################################\")\n",
        "print_text(text_val,0,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi7Mcmlv0lF5"
      },
      "source": [
        "# Functions for chat word conversion\n",
        "f = open(\"slang.txt\", \"r\")\n",
        "chat_words_str = f.read()\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXm_BGpc0lF6"
      },
      "source": [
        "# Chat word conversion\n",
        "# Training set\n",
        "text_train = text_train.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_train,0,10)\n",
        "\n",
        "print(\"********************************************************************************\")\n",
        "\n",
        "# Validation set\n",
        "text_val = text_val.apply(lambda text: chat_words_conversion(text))\n",
        "print_text(text_val,0,10)\n",
        "\n",
        "# Test set\n",
        "text_test = text_test.apply(lambda text: chat_words_conversion(text))\n",
        "# print_text(text_test,0,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1GiyEH0lF6"
      },
      "source": [
        "def ekphrasis_pipe(sentence):\n",
        "    cleaned_sentence = \" \".join(text_processor.pre_process_doc(sentence))\n",
        "    return cleaned_sentence"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGbdHvIX0lF7"
      },
      "source": [
        "# Training set\n",
        "text_train = text_train.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Training set completed.......\")\n",
        "#Validation set\n",
        "text_val = text_val.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Validation set completed.......\")\n",
        "#Test set\n",
        "text_test = text_test.apply(lambda text: ekphrasis_pipe(text))\n",
        "print(\"Test set completed.......\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWsDbRPo0lF7"
      },
      "source": [
        "# Finding length of longest array\n",
        "maxLen = len(max(text_train,key = lambda text: len(text.split(\" \"))).split(\" \"))\n",
        "print(maxLen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChqDR4Sd0lF7"
      },
      "source": [
        "u = lambda text: len(text.split(\" \"))\n",
        "sentence_lengths = []\n",
        "for x in text_train:\n",
        "    sentence_lengths.append(u(x))\n",
        "print(sorted(sentence_lengths)[-50:])\n",
        "print(len(sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzpL4nwN0lF8"
      },
      "source": [
        "is_humor = df_train[\"is_humor\"]\n",
        "humor_rating = df_train[\"humor_rating\"]\n",
        "humor_controversy = df_train[\"humor_controversy\"].astype(int)\n",
        "offense_rating = df_train[\"offense_rating\"]\n",
        "print(Counter(is_humor))\n",
        "print(Counter(humor_controversy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXVkP4b754M_"
      },
      "source": [
        "is_humor_val = df_val[\"is_humor\"]\r\n",
        "humor_rating_val = df_val[\"humor_rating\"]\r\n",
        "humor_controversy_val = df_val[\"humor_controversy\"].astype(int)\r\n",
        "offense_rating_val = df_val[\"offense_rating\"]\r\n",
        "print(Counter(is_humor_val))\r\n",
        "print(Counter(humor_controversy_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46UA6xX20lF9"
      },
      "source": [
        "from transformers import RobertaTokenizerFast, TFRobertaModel, TFBertModel, BertTokenizerFast, ElectraTokenizerFast, TFElectraModel, AlbertTokenizerFast, TFAlbertModel, XLNetTokenizerFast, TFXLNetModel, MPNetTokenizerFast, TFMPNetModel\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuwmTmCG4msU"
      },
      "source": [
        "# strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oujy0A_J0lF-"
      },
      "source": [
        "# Define tokenizer as per requirement\r\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hysOrYmC0lF-"
      },
      "source": [
        "text_train = list(text_train)\n",
        "text_val = list(text_val)\n",
        "text_test = list(text_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-C2A4qS0lF-"
      },
      "source": [
        "train_encodings = tokenizer(text_train, max_length=150, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "val_encodings = tokenizer(text_val, max_length=150, truncation=True, padding=\"max_length\", return_tensors='tf')\n",
        "test_encodings = tokenizer(text_test, max_length=150, truncation=True, padding=\"max_length\", return_tensors='tf')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVxYK_rC0lF-"
      },
      "source": [
        "print(np.shape(train_encodings[\"input_ids\"]))\n",
        "print(np.shape(val_encodings[\"input_ids\"]))\n",
        "print(np.shape(test_encodings[\"input_ids\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW5TfuDz0lF_"
      },
      "source": [
        "print(train_encodings[\"input_ids\"][0])\n",
        "print(\"***************************************************************************\")\n",
        "print(val_encodings[\"input_ids\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QUtPKK0lGA"
      },
      "source": [
        "def hahackathon_task_1(input_shape):\n",
        "    # Import model as required\n",
        "    model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "    layer = model.layers[0]\n",
        "    #Model\n",
        "    inputs = keras.Input(shape=input_shape, dtype='int32')\n",
        "    input_masks = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "    outputs = layer([inputs, input_masks])\n",
        "    output = outputs[0]\n",
        "    pooled_output = output[:, 0, :] # Use for bert, roberta, albert, mpnet, electra\n",
        "    # pooled_output = output[:, -1] # Use for XLNet\n",
        "\n",
        "    # Humour classification\n",
        "    is_humor = layers.Dropout(0.3)(pooled_output)\n",
        "    # is_humor = layers.Dense(128, activation=\"gelu\")(is_humor)\n",
        "    is_humor = layers.Dense(1, activation=\"sigmoid\")(is_humor)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs,input_masks], outputs=is_humor, name='Task_1_a')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4yJuStxv_jR"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqS2O31g34V4"
      },
      "source": [
        "with strategy.scope():\r\n",
        "    model = hahackathon_task_1((150,))\r\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=2e-5)\r\n",
        "    loss_fun = [\r\n",
        "          tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\n",
        "    ]\r\n",
        "    metric = [\r\n",
        "        tf.keras.metrics.BinaryAccuracy(),\r\n",
        "        tf.keras.metrics.Precision(),\r\n",
        "        tf.keras.metrics.Recall()\r\n",
        "    ]\r\n",
        "    model.compile(optimizer=optimizer, loss=loss_fun, metrics=metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyHqVLrVSutV"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfVFStvq0lGC"
      },
      "source": [
        "checkpoint = ModelCheckpoint(filepath='/content/task-1-a-model-name.{epoch:03d}.h5',\n",
        "                                 verbose = 0,\n",
        "                                 save_weights_only=True,\n",
        "                                 epoch=4)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll6THvmFyoIZ"
      },
      "source": [
        "count_humor = Counter(is_humor)\r\n",
        "print(count_humor)\r\n",
        "print(count_humor.keys())\r\n",
        "zero = count_humor[0]\r\n",
        "one = count_humor[1]\r\n",
        "total = zero + one\r\n",
        "print(\"Not humorous: \",zero)\r\n",
        "print(\"Humorous: \",one)\r\n",
        "print(\"Total: \",total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX0Se0j9yjm4"
      },
      "source": [
        "class_weight_ = {}\r\n",
        "maxi = max(zero, one)\r\n",
        "weight_for_0 = (maxi / (maxi + zero))\r\n",
        "weight_for_1 = (maxi / (maxi + one))\r\n",
        "\r\n",
        "class_weight_motivation = {0: weight_for_0, 1: weight_for_1}\r\n",
        "\r\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\r\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufyHiJkkJLCk"
      },
      "source": [
        "# albert\r\n",
        "history_task_1_a = model.fit(\r\n",
        "    x = [train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"]],\r\n",
        "    y = is_humor,\r\n",
        "    validation_data = ([val_encodings[\"input_ids\"],val_encodings[\"attention_mask\"]], is_humor_val),\r\n",
        "    callbacks = [checkpoint],\r\n",
        "    batch_size=16,\r\n",
        "    class_weight=class_weight_,\r\n",
        "    shuffle=True,\r\n",
        "    epochs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6vdbd1c0lGD"
      },
      "source": [
        "val_answer = model.predict([val_encodings[\"input_ids\"],val_encodings[\"attention_mask\"]])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-UA9iqHpt0k"
      },
      "source": [
        "val_answer = np.round(val_answer)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoNewJPvvk8b"
      },
      "source": [
        "val_answer = np.squeeze(val_answer, axis=-1)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFxSDSg0pnCg"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzU9RRo-pm7t"
      },
      "source": [
        "print(classification_report(is_humor_val, val_answer, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9PnLwm_YDm9"
      },
      "source": [
        "test_answer = model.predict([test_encodings[\"input_ids\"],test_encodings[\"attention_mask\"]])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWr7G2xCu3MA"
      },
      "source": [
        "test_answer = np.round(test_answer)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Cm-gb-au7vB"
      },
      "source": [
        "test_answer = np.squeeze(test_answer, axis=-1)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aR7wIiSvfB-"
      },
      "source": [
        "np.shape(test_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWyS0bxVu-Ww"
      },
      "source": [
        "test_id = df_test[\"id\"]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV9W3SQkvAWT"
      },
      "source": [
        "test_dict = {\r\n",
        "    \"id\" : test_id,\r\n",
        "    \"offense_rating\" : test_answer\r\n",
        "}"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXA0tn1IvCrh"
      },
      "source": [
        "df_test = pd.DataFrame(test_dict)\r\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWjvzzY8vEV1"
      },
      "source": [
        "df_test.to_csv('answer-1-a.csv', index=False)"
      ],
      "execution_count": 65,
      "outputs": []
    }
  ]
}